---
title: "36-315 Homework 12, Spring 2017"
author: "Your Name Here"
date: "Due Wednesday, May 3rd, 2017 (12pm ET) on Blackboard"
output: 
  html_document:
    toc:  true
    toc_float:  true
    code_folding:  hide
---

##  Homework 12:  Text and Time

***General instructions for all assignments***: 

+  Use this file as the template for your submission.  Delete the unnecessary text (e.g. this text, the problem statements, etc).  That said, keep the nicely formatted "Problem 1", "Problem 2", "a.", "b.", etc
+  Upload a single `R` Markdown file (named as:  [AndrewID]-HW09.Rmd -- e.g. "sventura-HW09.Rmd") to the Homework 09 submission section on Blackboard.  You do not need to upload the .html file.
+  The instructor and TAs will run your .Rmd file on their computers.  **If your .Rmd file does not knit on our computers, you will be automatically be deducted 10 points.**
+  Your file should contain the code to answer each question in its own code block.  Your code should produce plots/output that will be automatically embedded in the output (.html) file
+  Each answer must be supported by written statements (unless otherwise specified)
+  Include the name of anyone you collaborated with at the top of the assignment
+  Include the style guide you used below under Problem 0


***
***

#  Easy Problems

##  Problem 0

(4 points)

**Organization, Themes, and HTML Output**

a.  For all problems in this assignment, organize your output as follows:

+  Use code folding for all code.  See [here](http://blog.rstudio.org/2016/03/21/rmarkdown-v0-9-5/) for how to do this.
+  Use a floating table of contents.
+  Suppress all warning messages in your output by using `warning = FALSE` and `message = FALSE` in every code block.
+  Use tabs only if you see it fit to do so -- this is your choice.


b.  For all problems in this assignment, adhere to the following guidelines for your `ggplot` theme and use of color:

+  Do not use the default `ggplot()` color scheme.
+  For any bar chart or histogram, outline the bars (e.g. with `color = "black"`).
+  Do not use both red and green in the same plot, since a large proportion of the population is red-green colorblind.
+  Try to only use three colors (at most) in your themes.  In previous assignments, many students are using different colors for the axes, axis ticks, axis labels, graph titles, grid lines, background, etc.  This is unnecessary and only serves to make your graphs more difficult to read.  Use a more concise color scheme.
+  Make sure you use a white or gray background (preferably light gray if you use gray).
+  Make sure that titles, labels, and axes are in dark colors, so that they contrast well with the light colored background.
+  Only use color when necessary and when it enhances your graph.  For example, if you have a univariate bar chart, there's no need to color the bars different colors, since this is redundant.
+  In general, try to keep your themes (and written answers) professional.  Remember, you should treat these assignments as professional reports.


c.  Treat your submission as a formal report:

+  Use complete sentences when answering questions.  
+  Answer in the context of the problem.  
+  Treat your submission more as a formal "report", where you are providing details analyses to answer the research questions asked in the problems.


d.  What style guide are you using for this assignment?

```{r, message = F, warning = F}
library(tidyverse)
library(data.table)
library(forcats)

#  Simple theme with white background, legend at the bottom
my_theme <-  theme_bw() +
  theme(axis.text = element_text(size = 12, color = "indianred4"),
        text = element_text(size = 14, face = "bold", color = "darkslategrey"))

#  Colorblind-friendly color palette
my_colors <- c("#000000", "#56B4E9", "#E69F00", "#F0E442", "#009E73", "#0072B2", 
               "#D55E00", "#CC7947")

```

***
***


##  Problem 1

(5 points)

**World War II Data Visualization Video**

The video did a great job helping the viewer vizualize deaths during World War II! The visualizations were excecuted clearly, and often without any extra data ink, helping the viewer focus on the important information at hand. 

However, when visualizing deaths in bar charts, it was often difficult to see how many deaths there really were, since there was no y-axis label. In addition, in parts of the video where the bar charts were split, the space in between made it difficult to compare the deaths. 

Overall, this was a creative and helpful way to really see how many people died during World War II. 

***
***


##  Problem 2 {.tabset}


(4 points each; 16 points total)

**Time Series and Dates in `ggplot()`**

### a.  

```{r, warning = F, message = F}
library(tidyverse)
big_bike <- read_csv("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/big_bike.csv")

#  Add start_date variable to big_bike, and a bunch of other variables
big_bike <- mutate(big_bike,
                   start_date = as.Date(start_date),
                   birth_decade = paste0(substr(`birth year`, 1, 3), "0s"),
                   hour_of_day = as.integer(substr(time_of_day, 1, 2)),
                   am_or_pm = ifelse(hour_of_day < 12, "AM", "PM"),
                   day_of_week = weekdays(start_date),
                   less_than_30_mins = ifelse(tripduration < 1800, 
                                              "Short Trip", "Long Trip"),
                   weekend = ifelse(day_of_week %in% c("Saturday", "Sunday"),
                                    "Weekend", "Weekday"))

# dim(big_bike)
# sort(big_bike$start_date)[1]
# sort(big_bike$start_date)[nrow(big_bike)]
```

There are 105984 rows and 24 columns in the result. The minimum bike trip date is july 1st, 2013 and the maximum bike trip date is september 30th, 2016.


### b.


```{r, warning = F, message = F, fig.width = 6, fig.height = 2.5}
library(ggplot2)

#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day
trips_per_day <- big_bike %>%
  group_by(start_date) %>%
  summarize(n_trips = n())

#  Create a time series plot with the dates on the x-axis and the number of
#  trips per day on the y-axis
ggplot(trips_per_day, aes(x = start_date, y = n_trips)) + geom_line() + 
  scale_x_date() + geom_point() + 
  labs(x = "Start Date", y = "Number of Bike Trips", 
       main = "Number of Bike Trips Begun on Each Start Date") +  
  theme(plot.title = element_text(size = 18), 
        axis.title = element_text(size = 20)) + 
  my_theme
```

### c.  
```{r}
#which row of the start date column is equal to the max of the n_trips column

# trips_per_day$start_date[trips_per_day$n_trips==max(trips_per_day$n_trips)]
```
Removing geom_point from part b creates removes each point from the tiem series. The start dates are only connected with lines now. The start dates appear rather oscillating. It looks like the number of people who start on a certain day peak during a certain time each year (looks like it is the summer). September 15th, 2016 had the most bike trips. 


### d.

```{r, fig.height = 2.5, fig.width = 6}
#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day, split by usertype
trips_per_day_usertype <- big_bike %>%
  group_by(start_date, usertype) %>%
  summarize(n_trips = n())

#  Create a time series plot with the dates on the x-axis and the number of
#  trips per day on the y-axis, split by usertype
ggplot(trips_per_day_usertype, 
       aes(x = start_date, y = n_trips, color = usertype)) + 
  geom_line() + scale_x_date()  + 
  labs(x = "Start Date", y = "Number of Bike Trips", color = "User Type",
       main = paste0("Number of Bike Trips Begun on Each Start",
                     " Date Conditioned on User Type")) + 
  scale_color_manual(values=c("cornflowerblue", "goldenrod1")) +  
  theme(plot.title = element_text(size = 18),
        axis.title = element_text(size = 20)) + my_theme
```

It looks like subscribers ride much more than regular customers, which makes sense.


***
***


## Problem 3 {.tabset}

### Part A

```{r}
set.seed(315)
rand_ts <- rnorm(1000)
#acf(rand_ts, plot = FALSE)
```

The autocorrelations of the time series and itself at different lags all appear to be near 0 with the exception of the first entry, which has a correlation of 1. This makes sense, as this is the correlation of the time series at itself at a lag of 0 (i.e., no lag), which is the correlation of the time series with itself.

### Part B

```{r, fig.width=10}
acf(rand_ts, plot = TRUE)
```

There appear to be two autocorrelations that are significantly different from 0 in our plot at `lag=0` and `lag=7`. Again, we note that the significant difference for `lag=0` makes sense since the correlation of a time series with itself is equal to 1. The significant autocorrelation at `lag=7` makes less sense, since we know that `rand_ts` is a series of random draws from a Normal(0,1) distribution. While the correlation at `lag=7` is the largest in magnitude compared to the other lags, it is just barely over the threshhold for significance and is likely a result of random variance.

### Part C

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# code from problem 2
library(tidyverse)
big_bike <- read_csv("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/big_bike.csv")

#  Add start_date variable to big_bike, and a bunch of other variables
big_bike <- mutate(big_bike,
                   start_date = as.Date(start_date),
                   birth_decade = paste0(substr(`birth year`, 1, 3), "0s"),
                   hour_of_day = as.integer(substr(time_of_day, 1, 2)),
                   am_or_pm = ifelse(hour_of_day < 12, "AM", "PM"),
                   day_of_week = weekdays(start_date),
                   less_than_30_mins = ifelse(tripduration < 1800, 
                                              "Short Trip", "Long Trip"),
                   weekend = ifelse(day_of_week %in% c("Saturday", "Sunday"),
                                    "Weekend", "Weekday"))

library(ggplot2)

#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day
trips_per_day <- big_bike %>%
  group_by(start_date) %>%
  summarize(n_trips = n())

#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day, split by usertype
trips_per_day_usertype <- big_bike %>%
  group_by(start_date, usertype) %>%
  summarize(n_trips = n())
```

```{r}
bike_acf <- acf(trips_per_day$n_trips, plot = FALSE)
#names(bike_acf)
#bike_acf$acf
#bike_acf$lag
```

The autocorrelation results contain information on the estimated autocorrelations (`acf`), the type of correlation (`type`), the number of observations used in the time series (`n.used`), a three dimensional array containing the lags at which the autocorrelations are estimated (`lag`), the name of the series (`series`), and the series names for a multivariate time series (`snames`).

### Part D

```{r, fig.width=10}
par(mfrow = c(1, 2))
acf(filter(trips_per_day_usertype, usertype == "Customer")$n_trips, main = "Autocorrelations of NYC Citi Bike \n Customer Trips per Day")
acf(filter(trips_per_day_usertype, usertype == "Subscriber")$n_trips, main = "Autocorrelations of NYC Citi Bike \n Subscriber Trips per Day")
```

It appears that the time series of subscriber bike trips per day typically has higher autocorrelations, as the vertical lines representing autocorrelation tend to be larger at each lag compared to the customer time series.

### Part E

The highest significant, positive autocorrelations occur at lags 1, 7, 14, 21, and 28 for both time series. This means that an increase in the time series of bike trips per day of both Citi Bike customers and subscribers at lags 1, 7, 14, 21, and 28 results in a proportinate increase in the time series of number of bike trips per day without any lag. In other words, the current number of bike trips per day for both groups of users is highly correlated with the number of bike trips per day 1, 7, 14, 21, and 28 days in the past. We note that the most significant positive lags occur at multiples of 7. It is likely that the number of bike trips per day is similar for a given day of the week across weeks (e.g., there are a similar number of people riding bikes on any Saturday regardless of what week it is). In fact, we note that the farther away we get from a multiple of 7, the lower the autocorrelation of that particular lag becomes. The day directly before (lag 1) is also influencial.
***
***


#  Hard Problems

##  Problem 4

(14 points)

**Word Clouds and Tidy Text Mining**

a.  (2 points)  Read Sections 2, 2.1, and 2.5 of the Tidy Text Mining book from the Lecture 22 R Demo.  What does the `unnest_tokens()` function do?

b.  (4 points)  Load the Airline Tweets dataset from one of the first assignments.  What column contains the text of the tweets?  Run the following code and give an interpretation of the resulting word cloud (you may need to install the `tidytext` and `wordcloud` packages first):

```{r, warning = F, message = F}
#install.packages("tidytext")
#install.packages("wordcloud")
library(tidyverse)
library(tidytext)
library(wordcloud)
data(stop_words)

airline_tweets <- read_csv("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/Tweets.csv")

my_tweets <- dplyr::select(airline_tweets, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

c.  (8 points)  Create a separate wordcloud for each airline.  Arrange the results into a 2x3 grid.  Interpret the results:  Are there any words that are more/less common for certain airlines?


***
***


##  Problem 5

(4 points each; 32 points total)

**Moving Average Plots**


In Problem 2, we created a time series plot of the NYC bike dataset ("Big Bike"), visualizing the number of trips per day as a function of time.  Here, we're going to add a moving average line to the plot.

a.  Moving averages are calculated as follows:  Given a "window width" `ww`, the moving average at time point `tt` in the time series is:  `mean(my_time_series[(tt-ww+1):tt])`.  Write a function, called `moving_average` that calculates the moving average of a time series vector at a single time point.  The function should take three inputs:  the time series, the window width, and the time point at which you want the moving average.  The code is started for you below.

```{r}
#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
#  tt:  The point at which we want the moving average (leading up to that point)
moving_average <- function(tt, time_series, ww) {
  #  Throw an error if the window width is too big
  if (ww > length(time_series))  
    stop("Window width is greater than length of time series")
  
  #  If the window width is greater than the time point, return NA
  if (tt < ww)  return(NA)
  
  #  Your code here!
}
```


b.  Write a second function, called `get_moving_averages`, that calculates _all_ moving averages for a given time series vector.  Your function should take two inputs:  the time series and the window width.  This function should call your function from part (a).  The code is started for you below.

```{r}
#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
get_moving_averages <- function(time_series, ww) {
  #  Throw an error if the window width is too big
  if (ww > length(time_series))  
    stop("Window width is greater than length of time series")
  
  #  Your code here!
}
```


c.  Use your function in (b) to calculate a moving average vector for the `n_trips` variable in `trips_per_day` (use a window width of 14, or two weeks), and store this in a vector called `bike_moving_average_14`.  Print the first 20 elements of this vector out.  What do you notice about the first 13 observations?  Why is this

d.  Add the `bike_moving_average_14` variable to the `trips_per_day` dataset.  Recreate the plot from part (b) of Problem 2.  Now, add a second line showing the moving average in blue, with no points on the line.

e.  Interpret the plot in (d).  Does the moving average tend to line up closely with the actual observations?  Are there any systematic differences?  Between the moving average and the time series?  Which line is smoother, and why?

f.  Write two new functions, `weighted_moving_average` and `get_weighted_moving_averages`, that mimic the functions in (a) and (b), but use a weighted moving average instead.  Your functions should take the same parameters as their counterparts in parts (a) and (b), plus an additional parameter for the weighting scheme that should be used.  The code is started for you below.


```{r}
#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
#  tt:  The point at which 
#  weights:  the weights to be used in the moving average
#  Note:  length(weights) should always equal ww!
weighted_moving_average <- function(tt, time_series, ww, weights = NULL) {
  #  Throw an error if the window width is too big
  if (ww > length(time_series))  
    stop("Window width is greater than length of time series")
  
  #  If weights are not specified, use standard weights
  if (is.null(weights))  weights <- rep(1/ww, ww)
  
  #  Throw an error if the window width is too big
  if (length(weights) != ww)  
    stop("Weights should have the same length as the window width")
  
  #  If the window width is greater than the time point, return NA
  if (tt < ww)  return(NA)
  
  #  Standardize the weights so they sum to 1
  weights <- weights / sum(weights)
  
  #  Your code here!
}


#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
#  weights:  the weights to be used in the moving average
#  Note:  length(weights) should always equal ww!
get_weighted_moving_averages <- function(time_series, ww, weights) {
  #  Throw an error if the window width is too big
  if(ww > length(time_series))  stop("Window width is greater than length of time series")
  
  #  If weights are not specified, use standard weights
  if (is.null(weights))  weights <- rep(1/ww, ww)
  
  #  Throw an error if the window width is too big
  if (length(weights) != ww)  
    stop("Weights should have the same length as the window width")
  
  #  Standardize the weights so they sum to 1
  weights <- weights / sum(weights)
  
  #  Your code here!
}
```

g.  Calculate the weighted moving average (window width = 7) of the number of trips per day.  Use a weighting scheme that weights the most recent observation 5 times as much as the first 5 observations, and the second most recent observation 3 times as much as the other 5, e.g. `weights = c(1,1,1,1,1,3,5)`.  Add the result to the `trips_per_day` data.frame.  Recreate your plot from (d), and add a red, dashed line (specify `linetype`) showing the weighted moving average on the plot.

h.  Describe the plot from (g).  In paricular, comment on any differences between the two moving averages.


***
***

##  Problem 6

(20 points)

**Create `ggplot()` ACF Plots**

Earlier in this assignment, we learned about the `acf()` function and autocorrelation plots.  `acf()` uses base-R graphics.  Your task in this problem is to create a `ggplot()` version of autocorrelation plots.  Specifically, you should:

+  Write a function that takes a time series as input (e.g. the `n_trips` column of `trips_per_day`, or the `rand_ts` from HW12).
+  The function should compute the autocorrelations for the time series, and store them in a new data frame with one column indicating the lag, and another columns indicating the autocorrelation at that lag.
+  Use `geom_line()`, `geom_segment()`, `geom_hline()`, and other `ggplot()` geometries to draw a graph that mimics the autocorrelation plots from `acf()`.  That is, you should have vertical lines indicating the autocorrelation at each lag, and blue horizontal lines plotted at the values $0 +/- 1.96 / \sqrt(n)$, where $n$ is the number of rows in the dataset.
+  The function should output a `ggplot()` object that has the default title and axis labels, so that these can be added to the object after calling the function.
+  The result should render a graph that looks similar to what you get from `acf()`.
+  You are allowed (and encouraged!) to use the `acf()` function to calculate the autocorrelations.

Demonstrate that your function works on the `rand_ts` object and the `trips_per_day` dataset.  Compare your graphs to the base-R `acf()` graph. 


***
***

#  Bonus Problems

See the BonusProblems assignment on Blackboard.


***
***
***
***
***
***


