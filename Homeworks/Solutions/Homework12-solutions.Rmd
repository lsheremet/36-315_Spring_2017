---
title: "36-315 Homework 12, Spring 2017"
author: "Your Name Here"
date: "Due Wednesday, May 3rd, 2017 (12pm ET) on Blackboard"
output: 
  html_document:
    toc:  true
    toc_float:  true
    code_folding:  hide
---

##  Homework 12:  Text and Time

***General instructions for all assignments***: 

+  Use this file as the template for your submission.  Delete the unnecessary text (e.g. this text, the problem statements, etc).  That said, keep the nicely formatted "Problem 1", "Problem 2", "a.", "b.", etc
+  Upload a single `R` Markdown file (named as:  [AndrewID]-HW09.Rmd -- e.g. "sventura-HW09.Rmd") to the Homework 09 submission section on Blackboard.  You do not need to upload the .html file.
+  The instructor and TAs will run your .Rmd file on their computers.  **If your .Rmd file does not knit on our computers, you will be automatically be deducted 10 points.**
+  Your file should contain the code to answer each question in its own code block.  Your code should produce plots/output that will be automatically embedded in the output (.html) file
+  Each answer must be supported by written statements (unless otherwise specified)
+  Include the name of anyone you collaborated with at the top of the assignment
+  Include the style guide you used below under Problem 0


***
***

#  Easy Problems

##  Problem 0

(4 points)

**Organization, Themes, and HTML Output**

a.  For all problems in this assignment, organize your output as follows:

+  Use code folding for all code.  See [here](http://blog.rstudio.org/2016/03/21/rmarkdown-v0-9-5/) for how to do this.
+  Use a floating table of contents.
+  Suppress all warning messages in your output by using `warning = FALSE` and `message = FALSE` in every code block.
+  Use tabs only if you see it fit to do so -- this is your choice.


b.  For all problems in this assignment, adhere to the following guidelines for your `ggplot` theme and use of color:

+  Do not use the default `ggplot()` color scheme.
+  For any bar chart or histogram, outline the bars (e.g. with `color = "black"`).
+  Do not use both red and green in the same plot, since a large proportion of the population is red-green colorblind.
+  Try to only use three colors (at most) in your themes.  In previous assignments, many students are using different colors for the axes, axis ticks, axis labels, graph titles, grid lines, background, etc.  This is unnecessary and only serves to make your graphs more difficult to read.  Use a more concise color scheme.
+  Make sure you use a white or gray background (preferably light gray if you use gray).
+  Make sure that titles, labels, and axes are in dark colors, so that they contrast well with the light colored background.
+  Only use color when necessary and when it enhances your graph.  For example, if you have a univariate bar chart, there's no need to color the bars different colors, since this is redundant.
+  In general, try to keep your themes (and written answers) professional.  Remember, you should treat these assignments as professional reports.


c.  Treat your submission as a formal report:

+  Use complete sentences when answering questions.  
+  Answer in the context of the problem.  
+  Treat your submission more as a formal "report", where you are providing details analyses to answer the research questions asked in the problems.


d.  What style guide are you using for this assignment?

```{r, message = F, warning = F}
library(tidyverse)
library(data.table)
library(forcats)

#  Simple theme with white background, legend at the bottom
my_theme <-  theme_bw() +
  theme(axis.text = element_text(size = 12, color = "indianred4"),
        text = element_text(size = 14, face = "bold", color = "darkslategrey"))

#  Colorblind-friendly color palette
my_colors <- c("#000000", "#56B4E9", "#E69F00", "#F0E442", "#009E73", "#0072B2", 
               "#D55E00", "#CC7947")

```

***
***


##  Problem 1

(5 points)

**World War II Data Visualization Video**

The video did a great job helping the viewer vizualize deaths during World War II! The visualizations were excecuted clearly, and often without any extra data ink, helping the viewer focus on the important information at hand. 

However, when visualizing deaths in bar charts, it was often difficult to see how many deaths there really were, since there was no y-axis label. In addition, in parts of the video where the bar charts were split, the space in between made it difficult to compare the deaths. 

Overall, this was a creative and helpful way to really see how many people died during World War II. 

***
***


##  Problem 2 {.tabset}


(4 points each; 16 points total)

**Time Series and Dates in `ggplot()`**

### a.  

```{r, warning = F, message = F}
library(tidyverse)
big_bike <- read_csv("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/big_bike.csv")

#  Add start_date variable to big_bike, and a bunch of other variables
big_bike <- mutate(big_bike,
                   start_date = as.Date(start_date),
                   birth_decade = paste0(substr(`birth year`, 1, 3), "0s"),
                   hour_of_day = as.integer(substr(time_of_day, 1, 2)),
                   am_or_pm = ifelse(hour_of_day < 12, "AM", "PM"),
                   day_of_week = weekdays(start_date),
                   less_than_30_mins = ifelse(tripduration < 1800, 
                                              "Short Trip", "Long Trip"),
                   weekend = ifelse(day_of_week %in% c("Saturday", "Sunday"),
                                    "Weekend", "Weekday"))

# dim(big_bike)
# sort(big_bike$start_date)[1]
# sort(big_bike$start_date)[nrow(big_bike)]
```

There are 105984 rows and 24 columns in the result. The minimum bike trip date is july 1st, 2013 and the maximum bike trip date is september 30th, 2016.


### b.


```{r, warning = F, message = F, fig.width = 6, fig.height = 2.5}
library(ggplot2)

#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day
trips_per_day <- big_bike %>%
  group_by(start_date) %>%
  summarize(n_trips = n())

#  Create a time series plot with the dates on the x-axis and the number of
#  trips per day on the y-axis
ggplot(trips_per_day, aes(x = start_date, y = n_trips)) + geom_line() + 
  scale_x_date() + geom_point() + 
  labs(x = "Start Date", y = "Number of Bike Trips", 
       main = "Number of Bike Trips Begun on Each Start Date") +  
  theme(plot.title = element_text(size = 18), 
        axis.title = element_text(size = 20)) + 
  my_theme
```

### c.  
```{r}
#which row of the start date column is equal to the max of the n_trips column

# trips_per_day$start_date[trips_per_day$n_trips==max(trips_per_day$n_trips)]
```
Removing geom_point from part b creates removes each point from the tiem series. The start dates are only connected with lines now. The start dates appear rather oscillating. It looks like the number of people who start on a certain day peak during a certain time each year (looks like it is the summer). September 15th, 2016 had the most bike trips. 


### d.

```{r, fig.height = 2.5, fig.width = 6}
#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day, split by usertype
trips_per_day_usertype <- big_bike %>%
  group_by(start_date, usertype) %>%
  summarize(n_trips = n())

#  Create a time series plot with the dates on the x-axis and the number of
#  trips per day on the y-axis, split by usertype
ggplot(trips_per_day_usertype, 
       aes(x = start_date, y = n_trips, color = usertype)) + 
  geom_line() + scale_x_date()  + 
  labs(x = "Start Date", y = "Number of Bike Trips", color = "User Type",
       main = paste0("Number of Bike Trips Begun on Each Start",
                     " Date Conditioned on User Type")) + 
  scale_color_manual(values=c("cornflowerblue", "goldenrod1")) +  
  theme(plot.title = element_text(size = 18),
        axis.title = element_text(size = 20)) + my_theme
```

It looks like subscribers ride much more than regular customers, which makes sense.


***
***


## Problem 3 {.tabset}

### Part A

```{r}
set.seed(315)
rand_ts <- rnorm(1000)
#acf(rand_ts, plot = FALSE)
```

The autocorrelations of the time series and itself at different lags all appear to be near 0 with the exception of the first entry, which has a correlation of 1. This makes sense, as this is the correlation of the time series at itself at a lag of 0 (i.e., no lag), which is the correlation of the time series with itself.

### Part B

```{r, fig.width=10}
acf(rand_ts, plot = TRUE)
```

There appear to be two autocorrelations that are significantly different from 0 in our plot at `lag=0` and `lag=7`. Again, we note that the significant difference for `lag=0` makes sense since the correlation of a time series with itself is equal to 1. The significant autocorrelation at `lag=7` makes less sense, since we know that `rand_ts` is a series of random draws from a Normal(0,1) distribution. While the correlation at `lag=7` is the largest in magnitude compared to the other lags, it is just barely over the threshhold for significance and is likely a result of random variance.

### Part C

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# code from problem 2
library(tidyverse)
big_bike <- read_csv("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/big_bike.csv")

#  Add start_date variable to big_bike, and a bunch of other variables
big_bike <- mutate(big_bike,
                   start_date = as.Date(start_date),
                   birth_decade = paste0(substr(`birth year`, 1, 3), "0s"),
                   hour_of_day = as.integer(substr(time_of_day, 1, 2)),
                   am_or_pm = ifelse(hour_of_day < 12, "AM", "PM"),
                   day_of_week = weekdays(start_date),
                   less_than_30_mins = ifelse(tripduration < 1800, 
                                              "Short Trip", "Long Trip"),
                   weekend = ifelse(day_of_week %in% c("Saturday", "Sunday"),
                                    "Weekend", "Weekday"))

library(ggplot2)

#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day
trips_per_day <- big_bike %>%
  group_by(start_date) %>%
  summarize(n_trips = n())

#  Summarize the big_bike, creating a new data.frame that includes the number 
#  of trips taken on each day, split by usertype
trips_per_day_usertype <- big_bike %>%
  group_by(start_date, usertype) %>%
  summarize(n_trips = n())
```

```{r}
bike_acf <- acf(trips_per_day$n_trips, plot = FALSE)
#names(bike_acf)
#bike_acf$acf
#bike_acf$lag
```

The autocorrelation results contain information on the estimated autocorrelations (`acf`), the type of correlation (`type`), the number of observations used in the time series (`n.used`), a three dimensional array containing the lags at which the autocorrelations are estimated (`lag`), the name of the series (`series`), and the series names for a multivariate time series (`snames`).

### Part D

```{r, fig.width=10}
par(mfrow = c(1, 2))
acf(filter(trips_per_day_usertype, usertype == "Customer")$n_trips, main = "Autocorrelations of NYC Citi Bike \n Customer Trips per Day")
acf(filter(trips_per_day_usertype, usertype == "Subscriber")$n_trips, main = "Autocorrelations of NYC Citi Bike \n Subscriber Trips per Day")
```

It appears that the time series of subscriber bike trips per day typically has higher autocorrelations, as the vertical lines representing autocorrelation tend to be larger at each lag compared to the customer time series.

### Part E

The highest significant, positive autocorrelations occur at lags 1, 7, 14, 21, and 28 for both time series. This means that an increase in the time series of bike trips per day of both Citi Bike customers and subscribers at lags 1, 7, 14, 21, and 28 results in a proportinate increase in the time series of number of bike trips per day without any lag. In other words, the current number of bike trips per day for both groups of users is highly correlated with the number of bike trips per day 1, 7, 14, 21, and 28 days in the past. We note that the most significant positive lags occur at multiples of 7. It is likely that the number of bike trips per day is similar for a given day of the week across weeks (e.g., there are a similar number of people riding bikes on any Saturday regardless of what week it is). In fact, we note that the farther away we get from a multiple of 7, the lower the autocorrelation of that particular lag becomes. The day directly before (lag 1) is also influencial.
***
***


#  Hard Problems

##  Problem 4 {.tabset}

(16 points)


### a. 

The `unnest_tokens()` function extracts individual words, or 'tokens', from text and formats it into a tidy data structure which has one token per document per row.  In addition, the function converts words to lowercase and removes punctuation.

### b. 

The column `text` contains the tweets.  As expected with a dataset about airlines, words frequently tweeted were 'flight', 'cancelled', 'hours', 'time', 'delayed', and the names of different airlines.  


```{r, warning = F, message = F}
#install.packages("tidytext")
#install.packages("wordcloud")
library(tidytext)
library(wordcloud)
library(dplyr)
library(data.table)
data(stop_words)

airline_tweets <- fread("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/Tweets.csv")

my_tweets <- dplyr::select(airline_tweets, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
title(main = list("Word Cloud for Airline Tweets", col = 'blue'))
```

### c.

```{r, warning=FALSE, message=FALSE}
virgin_america <- filter(airline_tweets, airline == "Virgin America")
united <- filter(airline_tweets, airline == "United")
southwest <- filter(airline_tweets, airline == "Southwest")
delta <- filter(airline_tweets, airline == "Delta")
us_airways <- filter(airline_tweets, airline == "US Airways")
american <- filter(airline_tweets, airline == "American")

par(mfrow = c(2,3))

va_wordcloud <- dplyr::select(virgin_america, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 75))
title(main = list("Virgin America", col = 'blue'))

united_wordcloud <- dplyr::select(united, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 75))
title(main = list("United Airlines", col = 'blue'))

southwest_wordcloud <- dplyr::select(southwest, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 75))
title(main = list("Southwest", col = 'blue'))

delta_wordcloud <- dplyr::select(delta, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 75))
title(main = list("Delta Airlines", col = 'blue'))

usairways_wordcloud <- dplyr::select(us_airways, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 75))
title(main =  list("US Airways", col = 'blue'))

american_wordcloud <- dplyr::select(american, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 75, main = "Title")) 
title(main = list("American Airlines", col = 'blue'))
```

For each airline, frequently tweeted words were 'flight' and the name of the airline.  'Cancelled' was more frequently tweeted with regards to Southwest, American Airlines and US Airways.  In addition, 'jetblue' was frequently tweeted with regards to Delta Airlines. 


***
***


##  Problem 5 {.tabset}

(4 points each; 32 points total)

**Moving Average Plots**


### a. 

```{r}
#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
#  tt:  The point at which we want the moving average (leading up to that point)
moving_average <- function(tt, time_series, ww) {
  #  Throw an error if the window width is too big
  if (ww > length(time_series))  
    stop("Window width is greater than length of time series")
  
  #  If the window width is greater than the time point, return NA
  if (tt < ww)  return(NA)
  
  #  Your code here!
  return (mean(time_series[(tt-ww+1):tt]))
}
```

### b.

```{r}
#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
get_moving_averages <- function(time_series, ww) {
  #  Throw an error if the window width is too big
  if (ww > length(time_series))  
    stop("Window width is greater than length of time series")
  
  #  Your code here!
  return (sapply(1:length(time_series), moving_average, time_series, ww))
}
```

### c.  

```{r, warning = FALSE, message = FALSE}
# Load in the data
bike_data <- read_csv("https://raw.githubusercontent.com/sventura/315-code-and-datasets/master/data/big_bike.csv")

#  Add start_date variable to bike_data
bike_data$start_date <- as.Date(sapply(strsplit(as.character(bike_data$starttime), " "), 
                                       "[[", 1), 
                                format = "%Y-%m-%d")

library(dplyr)
library(ggplot2)

#  Summarize the bike_data, creating a new data.frame that includes the number 
#  of trips taken on each day
by_date <- group_by(bike_data, start_date)
trips_per_day <- summarize(by_date, n_trips = n())
trips_per_day <- na.omit(trips_per_day)

# Get moving averages for n_trips
bike_moving_average_14 <- get_moving_averages(trips_per_day$n_trips, 14)
head(bike_moving_average_14, 20)
```

The first 13 observations are NA because the window width is 14, so there is not enough past data to calculate a moving average at these times. 

### d. 

```{r, warning = FALSE}
# Add variable to dataset
trips_per_day$bike_moving_average_14 <- bike_moving_average_14

#  Create a time series plot with the dates on the x-axis and the number of
#  trips per day on the y-axis
# Add moving average line 
ggplot(trips_per_day, aes(x = start_date, y = n_trips)) + geom_line(aes(colour = "Original Data")) + 
  scale_x_date() + geom_point() +
  geom_line(aes(x = start_date, y = bike_moving_average_14, colour = "Moving Average")) +
  scale_colour_manual("", breaks = c("Original Data", "Moving Average"), 
                      values = c("blue", "black")) + 
  ggtitle("Number of Bike Trips in December")+ 
  labs(x = "Start Date", y = "Number of Trips")
```


###e.  

The moving average always has a lag from the original data. That is, the moving average follows the same pattern as the data, but does so a few days later.  This is because it is looking at the past 14 days of data to calculate its current value, so it is more similar to the past data than the future. 

We could redefine the moving average to take the 14 (or $w$) closest observations in either direction, rather than the 14 (or $w$) previous observations to eliminate this issue.

### f.  

```{r}
#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
#  tt:  The point at which 
#  weights:  the weights to be used in the moving average
#  Note:  length(weights) should always equal ww!
weighted_moving_average <- function(tt, time_series, ww, weights = NULL) {
  #  Throw an error if the window width is too big
  if (ww > length(time_series))  
    stop("Window width is greater than length of time series")
  
  #  If weights are not specified, use standard weights
  if (is.null(weights))  weights <- rep(1/ww, ww)
  
  #  Throw an error if the window width is too big
  if (length(weights) != ww)  
    stop("Weights should have the same length as the window width")
  
  #  If the window width is greater than the time point, return NA
  if (tt < ww)  return(NA)
  
  #  Standardize the weights so they sum to 1
  weights <- weights / sum(weights)
  
  #  Your code here!
  return (sum(weights * time_series[(tt-ww+1):tt]))
}


#  time_series:  A vector containing the time series values
#  ww:  The window width for the moving average
#  weights:  the weights to be used in the moving average
#  Note:  length(weights) should always equal ww!
get_weighted_moving_averages <- function(time_series, ww, weights) {
  #  Throw an error if the window width is too big
  if(ww > length(time_series))  stop("Window width is greater than length of time series")
  
  #  If weights are not specified, use standard weights
  if (is.null(weights))  weights <- rep(1/ww, ww)
  
  #  Throw an error if the window width is too big
  if (length(weights) != ww)  
    stop("Weights should have the same length as the window width")
  
  #  Standardize the weights so they sum to 1
  weights <- weights / sum(weights)
  
  #  Your code here!
  return(sapply(1:length(time_series), weighted_moving_average, time_series, ww, weights))
}
```


### g. 

```{r, warning = FALSE, message = FALSE}
# Calculate weighted moving average of trips per day
weight_MA <- get_weighted_moving_averages(trips_per_day$n_trips, 3, weights = c(1, 1, 3))
trips_per_day$weight_MA <- weight_MA

#  Create a time series plot with the dates on the x-axis and the number of
#  trips per day on the y-axis
# Add moving average lines
ggplot(trips_per_day, aes(x = start_date, y = n_trips)) + 
  geom_line(aes(colour = "Original Data")) + 
  scale_x_date() + geom_point() +
  geom_line(aes(x = start_date, y = bike_moving_average_14, colour = "Moving Average")) +
  geom_line(aes(x = start_date, y = weight_MA, colour = "Weighted Moving Average"), linetype = 2) +
  scale_color_manual(values = c("Original Data" = "black", "Moving Average" = "blue",
                                "Weighted Moving Average" = "red")) +
  ggtitle("Number of Bike Trips in December") + 
  labs(x = "Start Date", y = "Number of Trips")
```

### h.  

The weighted moving average is closer to the original data, as it weighted the more recent data higher and so takes less time to react to changes in the data since it puts more weight on them. Both moving averages are also smoother than the original data and do not have as high peaks and valleys since it is averaging over 3 time periods. 


***
***

##  Problem 6 {.tabset}

(20 points)

**Create `ggplot()` ACF Plots**

### code

```{r, message = F, warning = F}
acf_plot <- function(time_series){
  acf_data = acf(time_series, plot = F)
  acf_df = data.frame(acf_data$lag, acf_data$acf)
  colnames(acf_df) = c("lag", "autocorrelation")
  line1 = 0 + 1.96 / sqrt(length(time_series))
  line2 = 0 - 1.96 / sqrt(length(time_series))
  acf_graph <- ggplot(data = acf_df) + 
    geom_segment(aes(x = lag, y = 0, xend = lag, yend = autocorrelation)) + 
    geom_hline(yintercept = line1, color = "blue", linetype = "dashed") + 
    geom_hline(yintercept = line2, color = "blue", linetype = "dashed") + 
    geom_hline(yintercept = 0, color = "black")
  return(acf_graph)}
```


### comparison with acf() in rand_ts

```{r}
acf_plot(rnorm(1000)) + labs(
  title = "Autocorrelation of random time series - rnorm(1000)",
  x = "Lag",
  y = "Autocorrelation"
)

acf(rnorm(1000), main = "Autocorrelation of random time series - rnorm(1000)")
```

### comparison with acf() in bike data

```{r}
acf_plot(trips_per_day$n_trips) + labs(
  title = "Autocorrelation of number of trips per day from NYC bike dataset",
  x = "Lag",
  y = "Autocorrelation"
)

acf(trips_per_day$n_trips, main = "Autocorrelation of number of trips per day from NYC bike dataset")

```

***
***

#  Bonus Problems

See the BonusProblems assignment on Blackboard.


***
***
***
***
***
***


